---

title: Title
keywords: fastai
sidebar: home_sidebar

summary: "summary"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 00.Preface.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Hands-on-Machine-Learning-with-Scikit-Learn,-Keras,-and-TensorFlow">Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow<a class="anchor-link" href="#Hands-on-Machine-Learning-with-Scikit-Learn,-Keras,-and-TensorFlow">&#182;</a></h1><h2 id="Concepts,-Tools,-and-Techniques-to-Build-Intelligent-Systems">Concepts, Tools, and Techniques to Build Intelligent Systems<a class="anchor-link" href="#Concepts,-Tools,-and-Techniques-to-Build-Intelligent-Systems">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Table-of-Contents">Table of Contents<a class="anchor-link" href="#Table-of-Contents">&#182;</a></h3><h3 id="0.-Preface">0. Preface<a class="anchor-link" href="#0.-Preface">&#182;</a></h3><ul>
<li>[x] The Machine Learning Tsunami</li>
<li>[x] Machine Learning in your Projects</li>
<li>[x] Objective &amp; Approach</li>
<li>[x] Prerequesites</li>
<li>[x] Roadmap</li>
<li>[x] Changes in the Second Edition</li>
<li>[x] Other Resources</li>
<li>[x] Conventions used in this book</li>
<li>[x] Code Examples</li>
<li>[x] Using Code Examples</li>
</ul>
<hr>
<h3 id="1.-The-Fundamentals-of-Machine-Learning">1. The Fundamentals of Machine Learning<a class="anchor-link" href="#1.-The-Fundamentals-of-Machine-Learning">&#182;</a></h3><ul>
<li>The Machine Learning Landscape<ul>
<li>[ ] What is Machine Learning?</li>
<li>[ ] Why use Machine Learning?</li>
<li>[ ] Examples of Applications</li>
<li>[ ] Types of Machine Learning Systems</li>
<li>[ ] Main Challenges of Machine Learning</li>
<li>[ ] Testing and Validating</li>
</ul>
</li>
<li>End-to-End Machine Learning Project<ul>
<li>[ ] Working with Real Data</li>
<li>[ ] Look at the Big Picture</li>
<li>[ ] Get the Data</li>
<li>[ ] Discover and Visualize the Data to Gain Insights</li>
<li>[ ] Prepare the Data for Machine Learning Algorithms</li>
<li>[ ] Select and Train a Model</li>
<li>[ ] Fine-tune your Model</li>
<li>[ ] Launch, Monitor, and Maintain Your System</li>
</ul>
</li>
<li>Classification<ul>
<li>[ ] MNIST</li>
<li>[ ] Training a Binary Classifier</li>
<li>[ ] Performance Measures</li>
<li>[ ] Multiclass Classification</li>
<li>[ ] Error Analysis</li>
<li>[ ] Multilabel Classification</li>
<li>[ ] Multioutput Classification</li>
</ul>
</li>
<li>Training Models<ul>
<li>[ ] Linear Regression</li>
<li>[ ] Gradient Descent </li>
<li>[ ] Polynomial Regression</li>
<li>[ ] Learning Curves</li>
<li>[ ] Regularized Linear Models</li>
<li>[ ] Logistic Regression</li>
</ul>
</li>
<li>Support Vector Machines<ul>
<li>[ ] Linear SVM Classification</li>
<li>[ ] Nonlinear SVM Classification</li>
<li>[ ] SVM Regression</li>
<li>[ ] Under the Hood</li>
</ul>
</li>
<li>Decision Trees<ul>
<li>[ ] Training and Visualizing a Decision Tree</li>
<li>[ ] Making Predictions</li>
<li>[ ] Estimating Class Probabilities</li>
<li>[ ] The CART Training Algorithm</li>
<li>[ ] Computational Complexity </li>
<li>[ ] GINI Impurity or Entropy?</li>
<li>[ ] Regularization Hyperparameters</li>
<li>[ ] Regression</li>
<li>[ ] Instability</li>
</ul>
</li>
<li>Ensemble Learning and Random Forests<ul>
<li>[ ] Voting Classifiers</li>
<li>[ ] Bagging and Pasting</li>
<li>[ ] Random Patches and Random Subspaces</li>
<li>[ ] Random Forests</li>
<li>[ ] Boosting</li>
<li>[ ] Stacking</li>
</ul>
</li>
<li>Dimensionality Reduction<ul>
<li>[ ] The Curse of Dimensionality</li>
<li>[ ] Main Approaches for Dimensionality Reduction</li>
<li>[ ] PCA</li>
<li>[ ] Kernel PCA</li>
<li>[ ] LLE</li>
<li>[ ] Other dimensionality Reduction Techniques</li>
</ul>
</li>
<li>Unsupervised Learning Techniques<ul>
<li>[ ] Clustering</li>
<li>[ ] Gaussian Mixtures</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2.-Neural-Networks-&amp;-Deep-Learning">2. Neural Networks &amp; Deep Learning<a class="anchor-link" href="#2.-Neural-Networks-&amp;-Deep-Learning">&#182;</a></h3><ul>
<li>Introduction to Artificial Neural Networks with Keras<ul>
<li>[ ] From Biological to Artificial Neurons</li>
<li>[ ] Implementing MLPs with Keras</li>
<li>[ ] Fine-tuning Neural Network Hyperparameters</li>
</ul>
</li>
<li>Training Deep Neural Networks<ul>
<li>[ ] The Vanishing/Exploding Gradients Problems</li>
<li>[ ] Reusing pretrained layers</li>
<li>[ ] Faster Optimizers</li>
<li>[ ] Avoiding overfitting through regularization</li>
<li>[ ] Summary and Practical Guidelines</li>
</ul>
</li>
<li>Custom models and training with TensorFlow<ul>
<li>[ ] A Quick tour of TensorFlow</li>
<li>[ ] Using TensorFlow like NumPy</li>
<li>[ ] Customizing Models and Training Algorithms</li>
<li>[ ] TensorFlow functions and Graphs</li>
</ul>
</li>
<li>Loading and Preprocessing Data with TensorFlow<ul>
<li>[ ] The Data API</li>
<li>[ ] The TFRecord Format</li>
<li>[ ] Preprocessing the Input Features</li>
<li>[ ] TF Transform</li>
<li>[ ] The TensorFlow Datasets (TFDS) Project</li>
</ul>
</li>
<li>Deep Computer Vision using Convolutional Neural Networks<ul>
<li>[ ] The Architecture of the Visual Cortex</li>
<li>[ ] Convolutional Layers</li>
<li>[ ] Pooling Layers</li>
<li>[ ] CNN Architectures</li>
<li>[ ] Implementing a ResNet-34 CNN Using Keras</li>
<li>[ ] Using pretrained models from Keras</li>
<li>[ ] Pretrained models for Transfer Learning</li>
<li>[ ] Classification and Localization</li>
<li>[ ] Object Detection</li>
<li>[ ] Semantic Segmentation</li>
</ul>
</li>
<li>Processing Sequences Using RNNs and CNNs<ul>
<li>[ ] Recurrent Neurons and Layers</li>
<li>[ ] Training RNNs</li>
<li>[ ] Forecasting a Time Series</li>
<li>[ ] Handling Long Sequences</li>
</ul>
</li>
<li>Natural Language Processing with RNNs and Attention<ul>
<li>[ ] Generating Shakespearean Text Using a Character RNN</li>
<li>[ ] Sentiment Analysis</li>
<li>[ ] An Encoderâ€“Decoder Network for Neural Machine Translation</li>
<li>[ ] Attention Mechanisms</li>
<li>[ ] Recent Innovations in Language Models</li>
</ul>
</li>
<li>Representation Learning and Generative Learning Using Autoencoders and GANs<ul>
<li>[ ] Efficient Data Representations</li>
<li>[ ] Performing PCA with an Undercomplete Linear Autoencoder</li>
<li>[ ] Stacked Autoencoders</li>
<li>[ ] Convolutional Autoencoders</li>
<li>[ ] Recurrent Autoencoders</li>
<li>[ ] Denoising Autoencoders</li>
<li>[ ] Sparse Autoencoders</li>
<li>[ ] Variational Autoencoders</li>
<li>[ ] Generative Adversarial Networks</li>
</ul>
</li>
<li>Reinforcement Learning<ul>
<li>[ ] Learning to Optimize Rewards</li>
<li>[ ] Policy Search</li>
<li>[ ] Introduction to OpenAI Gym</li>
<li>[ ] Neural Network Policies</li>
<li>[ ] Evaluating Actions: The Credit Assignment Problem</li>
<li>[ ] Policy Gradients</li>
<li>[ ] Markov Decision Processes</li>
<li>[ ] Temporal Difference Learning</li>
<li>[ ] Q-Learning</li>
<li>[ ] Implementing Deep Q-Learning</li>
<li>[ ] Deep Q-Learning Variants</li>
<li>[ ] The TF-Agents Library</li>
<li>[ ] Overview of Some Popular RL Algorithms</li>
</ul>
</li>
<li>Training and Deploying TensorFlow Models at Scale<ul>
<li>[ ] Serving a TensorFlow Model</li>
<li>[ ] Deploying a Model to a Mobile or Embedded Device</li>
<li>[ ] Using GPUs to Speed Up Computations</li>
<li>[ ] Training Models Across Multiple Devices</li>
</ul>
</li>
</ul>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Preface">Preface<a class="anchor-link" href="#Preface">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>A deep neural networks is a very simplified version of our cerebral cortex.</li>
<li>This Books assumes you know close to nothing about machine learning.</li>
<li>We will cover a large number of machine learning techniques, from the most simpleset and widely used (Linear Regression) to deep learning techniques that win competitions.</li>
<li>We will be using production-ready Python frameworks<ul>
<li>Scikit-Learn</li>
<li>Keras</li>
<li>TensorFlow</li>
</ul>
</li>
<li>This book favors a hands-on approach through a series of working examples and just a little bit of theory.</li>
<li>Prerequesites<ul>
<li>Some Python programming experience</li>
<li>Familiarity with NumPy, Pandas, and Matplotlib</li>
<li>A reasonable understanding of college-level math (calculus, probability, Linear Algebra, and statistics)</li>
</ul>
</li>
<li>The first part of the book is mostly based on Scikit-Learn, while the 2nd part is using Keras/TensorFlow.</li>
<li>Moreover, most problems can be solved quite well using random forests and ensemble models, deep learning is best suited for complex problems with lots of data (Speech, Vision, Lanaguge, ..)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
</div>
 

