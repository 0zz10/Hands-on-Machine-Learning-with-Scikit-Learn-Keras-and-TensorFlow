{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp reduction\n",
    "# default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8. Dimensionality Reduction\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many ML problems will involve thousands or even millions of features per instance.\n",
    "- Not only all of these features make training extremly slow, but they can also make it much harder for an optimization method to find a good solution.\n",
    "- This problem is often referred to as the **curse of dimensionality**.\n",
    "- In real world problem, it is often possible to reduce the number of features considerably.\n",
    "    - Turning an intractable problem into a tractable one.\n",
    "- The goal is to remove the maximum number of features while minimizing information loss that relates to a specific task\n",
    "    - Task example â€” Classifying MNIST digits.\n",
    "- Reducing dimensionality does cause some information loss.\n",
    "- It also makes your pipeline a bit more complex and thus harder to maintain.\n",
    "- Dimensionality reduction is usually conducted to **speed up training**.\n",
    "- Dimensionality reduction is also extremly useful for data visualization.\n",
    "    - Taking it down to 2/3 dimensions for your data set will allow you visualize it in a 2/3D space.\n",
    "- DataViz is also important to communicate your findings to people who are not data scientists.\n",
    "    - In Particular, decision makers who will use your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this chapter we will:\n",
    "    - Discuss the curse of dimenstionality.\n",
    "    - Get a sense of what goes on in a high-dimenstional space.\n",
    "    - Consider the main two approaches to dimensionality reduction:\n",
    "        - Projection\n",
    "        - Manifold Learning\n",
    "    - Go through 3 popular dimensionality reduction techniques:\n",
    "        - PCA\n",
    "        - Kernel PCA\n",
    "        - LLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are so used to living in three dimensions\n",
    "    - 4 if you consider time, and a few more if you are a string theorist.\n",
    "- It turns out that many things behave very differently in high-dimensional spaces.\n",
    "- If you pick a random point in a unit square, it will have ~0.4% chance of being located at <0.001 from a border.\n",
    "- But in a 1,000-dimensional hyper-cube the probability is >99.999999%.\n",
    "    - Most points in a high-dimensional space are very close to the border.\n",
    "- Same goes to distances betweeen points, If you pick two random points in a lower dimensional space, they will be closer in comparison to picking them from a high-dimensional space.\n",
    "- **There is just plenty of space in a high-dimensional one!**\n",
    "- High-dimensional datasets are at risk of being too sparse.\n",
    "- The most dimensions a dataset has, the more risk it is to overfit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Approaches to Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In most real-world problems, training instances are not spread out uniformly across the dimensions.\n",
    "    - Many features are almost constant, while others are highly correlated.\n",
    "- As a result, all training instances like close to a much-lower dimensional **subspace** of the high-dimensional space.\n",
    "- Here is an example of that:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"static/imgs/subspace_projection.png\" />\n",
    "</div>\n",
    "\n",
    "- If we perpenducarly project every training instance into the subspace, we get a new 2D dataset represented as follows:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"static/imgs/2d_projection.png\" />\n",
    "</div>\n",
    "\n",
    "- However, projection is not always to best approach to dimensionality reduction.\n",
    "    - In many case the subspace may twist & turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A 2D manifold is a 2D shape that can be bent & twisted in a higher-dimensional space.\n",
    "- More generally, a d-dimensional manifold is part of an n-dimensional space (where d < n), that locally resembles a d-dimensional hyperplace.\n",
    "- Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie, this is called **Manifold Learning**.\n",
    "    - It relies on the *manifold assumption*, also called the *manifold hypothesis*.\n",
    "        - Which holds that most real world high-dimensional datasets lie close to a much lower-dimensional manifold.\n",
    "            - **This assumption is very often empirically observed**.\n",
    "- Thought experiment\n",
    "    - If you were to generate random images on a 28x28 grid, only a very small fraction of them would look like handwritten digits.\n",
    "    - In other words, the degrees of freedom available to you if you were to create a digit image are very low compared to the degree of freedom you have when you want to create any image you want (random).\n",
    "    - **These constraints tend to squeeze the dataset into a lower-dimensional manifold**.\n",
    "- An implicit additional assumption is that the task at hand (being regression or classification) would be much easier if conducted on the lower dimensional manifold space.\n",
    "    - This assumption does not always hold.\n",
    "    - Examples to follow:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"static/imgs/manifold_classification.png\" />\n",
    "</div>\n",
    "\n",
    "- In short, reducing the dimensionality of your dataset will speed up training, but it doesn't guarantee a simpler solution.\n",
    "    - It all depends on the dataset and the task at hand.\n",
    "- Now we will go through some of the most popular dimensionality reduction algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Principal Component Analysis is by far the most popular dimensionality reduction algorithm.\n",
    "- First, **It identifies the hyperplane that lies closest to the data**.\n",
    "- Then, **It projects the data into it**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preserving the Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We want to choose a hyperplace that most preserves tha variance within the data, following is attempted projections for 3 chosen hyperplanes (1D axis):\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:66%\" src=\"static/imgs/2D_variance_projection.png\" />\n",
    "</div>\n",
    "\n",
    "- It seems reasonable to select the axis that preserves the maximum amount of variance.\n",
    "    - As it will most likely lose less information than other projections.\n",
    "- Another way of looking at it is by choosing the solid line axis, we are minimizing the mean squared distance between the original points and their projections into the chosen axis.\n",
    "    - This is the rather simple idea behind PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA identifies the axis that accounts for the largest amount of variance in the training set.\n",
    "- It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance.\n",
    "- If we're considering a higher-dimensional dataset, PCA would also find a third axis, and a fourth, and a fifth, and so on...\n",
    "    - As many axes as the number of dimensions in the dataset.\n",
    "- The ith axis is called the ith **principal component** of the data.\n",
    "- So how can you find the principal components of a training set?\n",
    "- There is a standard matrix vectorization technique called *Singular Value Decomposition (SVD)*\n",
    "    - It can decompose the training set $X$ into $X=U \\Sigma V^T$\n",
    "    - $V$ contains the unit vectors that define all the principal components that we are looking for:\n",
    "    \n",
    "$$V=\n",
    "  \\begin{pmatrix}\n",
    "    \\vert & \\vert & \\dots & \\vert \\\\\n",
    "    c_1 & c_2 & \\dots & c_n \\\\\n",
    "    \\vert & \\vert & \\dots & \\vert \\\\\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- let's extract the principal components of a dataset using numpy's `svd` implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's start by generating some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,), (100,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.linspace(start=-1., stop=1., num=100)\n",
    "y = X + np.random.normal(size=100)/7.\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, y, c='blue')\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((X[..., None], y[..., None]), axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X - X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, Vt = np.linalg.svd(a=X_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.70338697, 0.71080712]), array([ 0.71080712, -0.70338697]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These two 2D points represent the unit vector points (origin at (0,0)) corresponding to the 2 principal components (axes that preserve variance).\n",
    "- PCA assumes that the dataset is centered around the origin, but scikit-learn implementation takes care of centering the data for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting down to $d$ Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once you have identified all the principal components, you can reduce the dimensionality of the dataset down to $d$ dimensions by projecting it onto the hyperplane defined by the first $d$ principal components.\n",
    "- Selecting this hyperplane ensures that the projection will preserve as much variance as possible.\n",
    "- To Project the training set into the hyperplane and obtain a reduced dataset $X_{d-proj}$ of dimensionality $d$:\n",
    "    - Compute the matrix multiplication of the training set matrix $X$ by the matrix $W_{d}$ \n",
    "        - $W_{d}$ is the matrix containing the first $d$ columns of $V$ representing the principal components.\n",
    "\n",
    "$$X_{d-proj}=XW_d$$\n",
    "\n",
    "- Let's do it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2D = X_centered.dot(W2)\n",
    "X2D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(pca.components_ == W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The ratio indicates the proportion of the dataset's variance lying along each principal component.\n",
    "- Let's take a look at it for our scikit-learn learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.98698474, 0.01301526]), 1.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_, np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Number of Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choose the number of dimensions that add up to a large portion of the variance that lied within the original dataset.\n",
    "    - Example: $>=95\\%$\n",
    "- Unless, ofcoures, you are reducing dimensionality to visualize the data.\n",
    "    - In that case you will want to reduce the data down to 2/3 axis.\n",
    "- Let's do it in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum = np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.argmax(cumsum >= .95) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then re-run PCA training using $d$.\n",
    "- But there is a much better option, that of specifying a float for `n_components` as the ratio of variance we want to preserve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PCA` for Compression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After dimensionality reduction, the training set takes much less space.\n",
    "- It is also possible to decompress the reduced dataset back to $784$ (in the case of MNIST) by applying **the inverse transformation of the PCA projection**.\n",
    "    - **This won't give you back the original data since 5% of the variance was lost while compressing**.\n",
    "    - But it will likely be close to the original dataset.\n",
    "- **The mean squared distance between the original dataset and the decompressed dataset is called the Reconstruction Error**.\n",
    "- Let's do it with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.fetch_openml(name='mnist_784', return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((46900, 784), (23100, 784), (46900,), (23100,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=154)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0].reshape(28, 28), cmap='binary')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_recovered[0].reshape(28, 28), cmap='binary')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Following is the equation of the inverse transformation:\n",
    "\n",
    "$$X_{recovered}=X_{d-proj}W_{d}^{T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use a stochastic algorithm called *Randomized PCA* that quickly finds an approximation of the first $d$ principal components.\n",
    "- Its computational complexity is $O(m \\times d^{2})+O(d^3)$ instead of SVD's $O(m \\times n^{2})+O(n^3)$.\n",
    "- So it's dramatically faster then SVD when $d << n$.\n",
    "- Let's use it with `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver='randomized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One problem with the previous implementations of PCA is that they require the whole training data to fit in memory.\n",
    "- Fortunately, **incremental PCA** algorithms have been developed.\n",
    "- They allow us to split the training set in mini-batches and feed them one at a time to the IPCA algorithm.\n",
    "- This is useful when having large training sets or doing online learning.\n",
    "- Let's experiment with incremental PCA using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can also mimic normal fitting behavior by using the `memap` class to store our training data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall that a linear decision boundary in a high-dimensional space corresponds to a non-linear decision boundary in the original low-dimensional space.\n",
    "- It turns out that the \"kernel trick\" can also be applied to PCA.\n",
    "- Making it possible to perform complex non-linear projections for dimensionality reduction.\n",
    "- It's often **good at preserving clusters of instances after projecting them**.\n",
    "- Let's use kPCA in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Kernel and Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Note: Use a GPU*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
