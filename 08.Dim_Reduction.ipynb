{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp reduction\n",
    "# default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8. Dimensionality Reduction\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many ML problems will involve thousands or even millions of features per instance.\n",
    "- Not only all of these features make training extremly slow, but they can also make it much harder for an optimization method to find a good solution.\n",
    "- This problem is often referred to as the **curse of dimensionality**.\n",
    "- In real world problem, it is often possible to reduce the number of features considerably.\n",
    "    - Turning an intractable problem into a tractable one.\n",
    "- The goal is to remove the maximum number of features while minimizing information loss that relates to a specific task\n",
    "    - Task example â€” Classifying MNIST digits.\n",
    "- Reducing dimensionality does cause some information loss.\n",
    "- It also makes your pipeline a bit more complex and thus harder to maintain.\n",
    "- Dimensionality reduction is usually conducted to **speed up training**.\n",
    "- Dimensionality reduction is also extremly useful for data visualization.\n",
    "    - Taking it down to 2/3 dimensions for your data set will allow you visualize it in a 2/3D space.\n",
    "- DataViz is also important to communicate your findings to people who are not data scientists.\n",
    "    - In Particular, decision makers who will use your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this chapter we will:\n",
    "    - Discuss the curse of dimenstionality.\n",
    "    - Get a sense of what goes on in a high-dimenstional space.\n",
    "    - Consider the main two approaches to dimensionality reduction:\n",
    "        - Projection\n",
    "        - Manifold Learning\n",
    "    - Go through 3 popular dimensionality reduction techniques:\n",
    "        - PCA\n",
    "        - Kernel PCA\n",
    "        - LLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are so used to living in three dimensions\n",
    "    - 4 if you consider time, and a few more if you are a string theorist.\n",
    "- It turns out that many things behave very differently in high-dimensional spaces.\n",
    "- If you pick a random point in a unit square, it will have ~0.4% chance of being located at <0.001 from a border.\n",
    "- But in a 1,000-dimensional hyper-cube the probability is >99.999999%.\n",
    "    - Most points in a high-dimensional space are very close to the border.\n",
    "- Same goes to distances betweeen points, If you pick two random points in a lower dimensional space, they will be closer in comparison to picking them from a high-dimensional space.\n",
    "- **There is just plenty of space in a high-dimensional one!**\n",
    "- High-dimensional datasets are at risk of being too sparse.\n",
    "- The most dimensions a dataset has, the more risk it is to overfit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Approaches to Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In most real-world problems, training instances are not spread out uniformly across the dimensions.\n",
    "    - Many features are almost constant, while others are highly correlated.\n",
    "- As a result, all training instances like close to a much-lower dimensional **subspace** of the high-dimensional space.\n",
    "- Here is an example of that:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"static/imgs/subspace_projection.png\" />\n",
    "</div>\n",
    "\n",
    "- If we perpenducarly project every training instance into the subspace, we get a new 2D dataset represented as follows:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"static/imgs/2d_projection.png\" />\n",
    "</div>\n",
    "\n",
    "- However, projection is not always to best approach to dimensionality reduction.\n",
    "    - In many case the subspace may twist & turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A 2D manifold is a 2D shape that can be bent & twisted in a higher-dimensional space.\n",
    "- More generally, a d-dimensional manifold is part of an n-dimensional space (where d < n), that locally resembles a d-dimensional hyperplace.\n",
    "- Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie, this is called **Manifold Learning**.\n",
    "    - It relies on the *manifold assumption*, also called the *manifold hypothesis*.\n",
    "        - Which holds that most real world high-dimensional datasets lie close to a much lower-dimensional manifold.\n",
    "            - **This assumption is very often empirically observed**.\n",
    "- Thought experiment\n",
    "    - If you were to generate random images on a 28x28 grid, only a very small fraction of them would look like handwritten digits.\n",
    "    - In other words, the degrees of freedom available to you if you were to create a digit image are very low compared to the degree of freedom you have when you want to create any image you want (random).\n",
    "    - **These constraints tend to squeeze the dataset into a lower-dimensional manifold**.\n",
    "- An implicit additional assumption is that the task at hand (being regression or classification) would be much easier if conducted on the lower dimensional manifold space.\n",
    "    - This assumption does not always hold.\n",
    "    - Examples to follow:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"static/imgs/manifold_classification.png\" />\n",
    "</div>\n",
    "\n",
    "- In short, reducing the dimensionality of your dataset will speed up training, but it doesn't guarantee a simpler solution.\n",
    "    - It all depends on the dataset and the task at hand.\n",
    "- Now we will go through some of the most popular dimensionality reduction algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Principal Component Analysis is by far the most popular dimensionality reduction algorithm.\n",
    "- First, **It identifies the hyperplane that lies closest to the data**.\n",
    "- Then, **It projects the data into it**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preserving the Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We want to choose a hyperplace that most preserves tha variance within the data, following is attempted projections for 3 chosen hyperplanes (1D axis):\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:66%\" src=\"static/imgs/2D_variance_projection.png\" />\n",
    "</div>\n",
    "\n",
    "- It seems reasonable to select the axis that preserves the maximum amount of variance.\n",
    "    - As it will most likely lose less information than other projections.\n",
    "- Another way of looking at it is by choosing the solid line axis, we are minimizing the mean squared distance between the original points and their projections into the chosen axis.\n",
    "    - This is the rather simple idea behind PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
