{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
    "> Concepts, Tools, and Techniques to Build Intelligent Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "### 0. Preface\n",
    "- [x] The Machine Learning Tsunami\n",
    "- [x] Machine Learning in your Projects\n",
    "- [x] Objective & Approach\n",
    "- [x] Prerequesites\n",
    "- [x] Roadmap\n",
    "- [x] Changes in the Second Edition\n",
    "- [x] Other Resources\n",
    "- [x] Conventions used in this book\n",
    "- [x] Code Examples\n",
    "- [x] Using Code Examples\n",
    "\n",
    "---\n",
    "### 1. The Fundamentals of Machine Learning\n",
    "- The Machine Learning Landscape\n",
    "    - [ ] What is Machine Learning?\n",
    "    - [ ] Why use Machine Learning?\n",
    "    - [ ] Examples of Applications\n",
    "    - [ ] Types of Machine Learning Systems\n",
    "    - [ ] Main Challenges of Machine Learning\n",
    "    - [ ] Testing and Validating\n",
    "- End-to-End Machine Learning Project\n",
    "    - [ ] Working with Real Data\n",
    "    - [ ] Look at the Big Picture\n",
    "    - [ ] Get the Data\n",
    "    - [ ] Discover and Visualize the Data to Gain Insights\n",
    "    - [ ] Prepare the Data for Machine Learning Algorithms\n",
    "    - [ ] Select and Train a Model\n",
    "    - [ ] Fine-tune your Model\n",
    "    - [ ] Launch, Monitor, and Maintain Your System\n",
    "- Classification\n",
    "    - [ ] MNIST\n",
    "    - [ ] Training a Binary Classifier\n",
    "    - [ ] Performance Measures\n",
    "    - [ ] Multiclass Classification\n",
    "    - [ ] Error Analysis\n",
    "    - [ ] Multilabel Classification\n",
    "    - [ ] Multioutput Classification\n",
    "- Training Models\n",
    "    - [ ] Linear Regression\n",
    "    - [ ] Gradient Descent \n",
    "    - [ ] Polynomial Regression\n",
    "    - [ ] Learning Curves\n",
    "    - [ ] Regularized Linear Models\n",
    "    - [ ] Logistic Regression\n",
    "- Support Vector Machines\n",
    "    - [ ] Linear SVM Classification\n",
    "    - [ ] Nonlinear SVM Classification\n",
    "    - [ ] SVM Regression\n",
    "    - [ ] Under the Hood\n",
    "- Decision Trees\n",
    "    - [ ] Training and Visualizing a Decision Tree\n",
    "    - [ ] Making Predictions\n",
    "    - [ ] Estimating Class Probabilities\n",
    "    - [ ] The CART Training Algorithm\n",
    "    - [ ] Computational Complexity \n",
    "    - [ ] GINI Impurity or Entropy?\n",
    "    - [ ] Regularization Hyperparameters\n",
    "    - [ ] Regression\n",
    "    - [ ] Instability\n",
    "- Ensemble Learning and Random Forests\n",
    "    - [ ] Voting Classifiers\n",
    "    - [ ] Bagging and Pasting\n",
    "    - [ ] Random Patches and Random Subspaces\n",
    "    - [ ] Random Forests\n",
    "    - [ ] Boosting\n",
    "    - [ ] Stacking\n",
    "- Dimensionality Reduction\n",
    "    - [ ] The Curse of Dimensionality\n",
    "    - [ ] Main Approaches for Dimensionality Reduction\n",
    "    - [ ] PCA\n",
    "    - [ ] Kernel PCA\n",
    "    - [ ] LLE\n",
    "    - [ ] Other dimensionality Reduction Techniques\n",
    "- Unsupervised Learning Techniques\n",
    "    - [ ] Clustering\n",
    "    - [ ] Gaussian Mixtures\n",
    "\n",
    "---\n",
    "### 2. Neural Networks & Deep Learning\n",
    "- Introduction to Artificial Neural Networks with Keras\n",
    "    - [ ] From Biological to Artificial Neurons\n",
    "    - [ ] Implementing MLPs with Keras\n",
    "    - [ ] Fine-tuning Neural Network Hyperparameters\n",
    "- Training Deep Neural Networks\n",
    "    - [ ] The Vanishing/Exploding Gradients Problems\n",
    "    - [ ] Reusing pretrained layers\n",
    "    - [ ] Faster Optimizers\n",
    "    - [ ] Avoiding overfitting through regularization\n",
    "    - [ ] Summary and Practical Guidelines\n",
    "- Custom models and training with TensorFlow\n",
    "    - [ ] A Quick tour of TensorFlow\n",
    "    - [ ] Using TensorFlow like NumPy\n",
    "    - [ ] Customizing Models and Training Algorithms\n",
    "    - [ ] TensorFlow functions and Graphs\n",
    "- Loading and Preprocessing Data with TensorFlow\n",
    "    - [ ] The Data API\n",
    "    - [ ] The TFRecord Format\n",
    "    - [ ] Preprocessing the Input Features\n",
    "    - [ ] TF Transform\n",
    "    - [ ] The TensorFlow Datasets (TFDS) Project\n",
    "- Deep Computer Vision using Convolutional Neural Networks\n",
    "    - [ ] The Architecture of the Visual Cortex\n",
    "    - [ ] Convolutional Layers\n",
    "    - [ ] Pooling Layers\n",
    "    - [ ] CNN Architectures\n",
    "    - [ ] Implementing a ResNet-34 CNN Using Keras\n",
    "    - [ ] Using pretrained models from Keras\n",
    "    - [ ] Pretrained models for Transfer Learning\n",
    "    - [ ] Classification and Localization\n",
    "    - [ ] Object Detection\n",
    "    - [ ] Semantic Segmentation\n",
    "- Processing Sequences Using RNNs and CNNs\n",
    "    - [ ] Recurrent Neurons and Layers\n",
    "    - [ ] Training RNNs\n",
    "    - [ ] Forecasting a Time Series\n",
    "    - [ ] Handling Long Sequences\n",
    "- Natural Language Processing with RNNs and Attention\n",
    "    - [ ] Generating Shakespearean Text Using a Character RNN\n",
    "    - [ ] Sentiment Analysis\n",
    "    - [ ] An Encoderâ€“Decoder Network for Neural Machine Translation\n",
    "    - [ ] Attention Mechanisms\n",
    "    - [ ] Recent Innovations in Language Models\n",
    "- Representation Learning and Generative Learning Using Autoencoders and GANs\n",
    "    - [ ] Efficient Data Representations\n",
    "    - [ ] Performing PCA with an Undercomplete Linear Autoencoder\n",
    "    - [ ] Stacked Autoencoders\n",
    "    - [ ] Convolutional Autoencoders\n",
    "    - [ ] Recurrent Autoencoders\n",
    "    - [ ] Denoising Autoencoders\n",
    "    - [ ] Sparse Autoencoders\n",
    "    - [ ] Variational Autoencoders\n",
    "    - [ ] Generative Adversarial Networks\n",
    "- Reinforcement Learning\n",
    "    - [ ] Learning to Optimize Rewards\n",
    "    - [ ] Policy Search\n",
    "    - [ ] Introduction to OpenAI Gym\n",
    "    - [ ] Neural Network Policies\n",
    "    - [ ] Evaluating Actions: The Credit Assignment Problem\n",
    "    - [ ] Policy Gradients\n",
    "    - [ ] Markov Decision Processes\n",
    "    - [ ] Temporal Difference Learning\n",
    "    - [ ] Q-Learning\n",
    "    - [ ] Implementing Deep Q-Learning\n",
    "    - [ ] Deep Q-Learning Variants\n",
    "    - [ ] The TF-Agents Library\n",
    "    - [ ] Overview of Some Popular RL Algorithms\n",
    "- Training and Deploying TensorFlow Models at Scale\n",
    "    - [ ] Serving a TensorFlow Model\n",
    "    - [ ] Deploying a Model to a Mobile or Embedded Device\n",
    "    - [ ] Using GPUs to Speed Up Computations\n",
    "    - [ ] Training Models Across Multiple Devices\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A deep neural networks is a very simplified version of our cerebral cortex.\n",
    "- This Books assumes you know close to nothing about machine learning.\n",
    "- We will cover a large number of machine learning techniques, from the most simpleset and widely used (Linear Regression) to deep learning techniques that win competitions.\n",
    "- We will be using production-ready Python frameworks\n",
    "    - Scikit-Learn\n",
    "    - Keras\n",
    "    - TensorFlow\n",
    "- This book favors a hands-on approach through a series of working examples and just a little bit of theory.\n",
    "- Prerequesites\n",
    "    - Some Python programming experience\n",
    "    - Familiarity with NumPy, Pandas, and Matplotlib\n",
    "    - A reasonable understanding of college-level math (calculus, probability, Linear Algebra, and statistics)\n",
    "- The first part of the book is mostly based on Scikit-Learn, while the 2nd part is using Keras/TensorFlow.\n",
    "- Moreover, most problems can be solved quite well using random forests and ensemble models, deep learning is best suited for complex problems with lots of data (Speech, Vision, Lanaguge, ..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
