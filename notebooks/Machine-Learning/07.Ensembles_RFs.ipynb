{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Ensemble Learning & Random Forests\n",
    "\n",
    "- Suppose you pose a random question to thousands of people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an **expert's answer** (*really*).\n",
    "    - This is called the *wisdom of the crowd*.\n",
    "- Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than the best individual predictor.\n",
    "- A group of predictors is called an ensemble. Thus this technique is called **ensemble learning**, and an ensemble learning algorithm is called an **Ensemble Method**.\n",
    "- As an example of an ensemble method, you can train a group of decision tree classifiers, each on a *random subset* of the training data. \n",
    "    - Such an ensemble of decision trees is called a **random forest**. \n",
    "    - Despite its simplicity, this is one of the most powerful machine learning algorithms available today.\n",
    "- You will often use ensemble methods near the end of a project, once you have already built a few good predictors, to combine them into an even better predictor.\n",
    "- In this chapter, we will discuss the most famous ensemble learning methods, including:\n",
    "    - **Bagging, Boosting, & Stacking**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "\n",
    "- Suppose you have trained a few classifiers, each achieving an 80% accuracy.\n",
    "- A very simple way to create an even better classifiers is to aggregate the predictions of all your classifiers and choose the prediction that is the most frequent.\n",
    "- This majority voting classifier is called a **Hard Voting** Classifier:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width: 50%;\" src=\"static/imgs/Hard_voting.png\"></div>\n",
    "\n",
    "- Somewhat surprisingly, this classifier achieves an even better accuracy than the best predictor in the ensemble.\n",
    "    - Even if each classifier is a weak learner (does slightly better then random guessing).\n",
    "        - Provided there are a sufficient number of weak learners and enough diversity.\n",
    "- Due to the law of large numbers, if you build an ensemble containing 1,000 classifiers with individual accuracies of $51%$ & trained for binary classification, If you predict the majority voting class, you can hope for up to $75%$ accuracy.\n",
    "    - This is only true if all classifiers are completely independent, making uncorrelated errors, which is clearly not the case because they are trained on the same data.\n",
    "- One way to get diverse classifiers is use different algorithms for each one of them & train them on different subset of the training data. \n",
    "- Let's implement a hard voting ensemble learner using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver='lbfgs')\n",
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "svm_clf = SVC(gamma='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=10000, noise=0.5)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6700, 2), (6700,), (3300, 2), (3300,))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rf_clf), ('svc', svm_clf)], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)), ('rf', RandomF...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=None, voting='hard', weights=None)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's take a look at the performance of each classifier + ensemble method on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8106060606060606\n",
      "RandomForestClassifier 0.8093939393939394\n",
      "SVC 0.8251515151515152\n",
      "VotingClassifier 0.8224242424242424\n"
     ]
    }
   ],
   "source": [
    "for clf in [log_clf, rf_clf, svm_clf, voting_clf]:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_hat = clf.predict(X_val)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There you have it! The voting classifier slightly outperforms the individual classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If all ensemble method learners can estimate class probabilities, you can average their probabilities per class then predict the class with the highest probability.\n",
    "    - This is called **Soft voting**.\n",
    "    - It often yields results better than hard voting because it weights confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging & Pasting\n",
    "\n",
    "- Another approach to having different algorithms trained on the same dataset is to have one algorithm but trained on random subsets of the training data.\n",
    "    - When subset sampling is performed with replacement, this is called **bagging**.\n",
    "    - When sampling is performed without replacement, this is called **Pasting**.\n",
    "- Bagging is showcased in the following figure:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width: 50%;\" src=\"static/imgs/bagging.png\"></div>\n",
    "\n",
    "- Once the ensemble method is done training, we can infer using *mode* for classification or *avg* for regression.\n",
    "- Each individual predictor has a higher bias than if it were trained on the whole training dataset, but aggregation reduces both bias and variance.\n",
    "- Because ensemble learners use separate different algorithms, they can be easily parallalizable, and that is why bagging and pasting are so popular, they scale very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging & Pasting in Scikit-Learn\n",
    "\n",
    "- Let's implement bagging in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=100, n_estimators=500, n_jobs=-1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = bag_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging models often result in a slightly more biased model than the base predictor, but because of the diversity of the training subsets, it has much smoother decision curves meaning it has usually a lower variance\n",
    "- The comparable/slightly higher bias comes from the fact that we are sampling with replacement, leading to points making it to multiple child learners.\n",
    "- following is a comparison\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/dt_bagging.png\"></div>\n",
    "\n",
    "- Overall, bagging generally produce better models, and this explains why it's very popular.\n",
    "    - but if you have compute time and power you can use cross validation to test both bagging and pasting for your case and decide which works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag Evaluation\n",
    "\n",
    "- With bagging, some instances will be sampled several times for several predictors, while others won't be sampled at all.\n",
    "- Only about 63% of the instances are sampled for each predictor.\n",
    "- The other 37% of instances not sampled are called Out-of-Bag (oob) instances. \n",
    "    - Note that they're not the same 37% for all predictors.\n",
    "- Since the individual predictor never sees the oob samples, it can be evaluated on the oob data without the need for a separate validation set.\n",
    "- You can evaluate the ensemble itself by averaging the oob scores of each predictor.\n",
    "- Let's do this using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=0.63, bootstrap=True, n_jobs=-1, oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=0.63, n_estimators=500, n_jobs=-1, oob_score=True,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8011940298507463"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify this estimation using the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = bag_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112121212121212"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Close enough!\n",
    "- The oob decision function is also available using scikit-learn's API, since our classifier is a decision tree then the decision function is a ratio that can be expressed as a probability, let's take a look at it for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45      , 0.55      ],\n",
       "       [0.26538462, 0.73461538],\n",
       "       [0.01111111, 0.98888889],\n",
       "       ...,\n",
       "       [0.05882353, 0.94117647],\n",
       "       [0.6741573 , 0.3258427 ],\n",
       "       [0.99640288, 0.00359712]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Patches & Random Subspaces\n",
    "\n",
    "- ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
