{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Ensemble Learning & Random Forests\n",
    "\n",
    "- Suppose you pose a random question to thousands of people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an **expert's answer** (*really*).\n",
    "    - This is called the *wisdom of the crowd*.\n",
    "- Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than the best individual predictor.\n",
    "- A group of predictors is called an ensemble. Thus this technique is called **ensemble learning**, and an ensemble learning algorithm is called an **Ensemble Method**.\n",
    "- As an example of an ensemble method, you can train a group of decision tree classifiers, each on a *random subset* of the training data. \n",
    "    - Such an ensemble of decision trees is called a **random forest**. \n",
    "    - Despite its simplicity, this is one of the most powerful machine learning algorithms available today.\n",
    "- You will often use ensemble methods near the end of a project, once you have already built a few good predictors, to combine them into an even better predictor.\n",
    "- In this chapter, we will discuss the most famous ensemble learning methods, including:\n",
    "    - **Bagging, Boosting, & Stacking**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "\n",
    "- Suppose you have trained a few classifiers, each achieving an 80% accuracy.\n",
    "- A very simple way to create an even better classifiers is to aggregate the predictions of all your classifiers and choose the prediction that is the most frequent.\n",
    "- This majority voting classifier is called a **Hard Voting** Classifier:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width: 50%;\" src=\"static/imgs/Hard_voting.png\"></div>\n",
    "\n",
    "- Somewhat surprisingly, this classifier achieves an even better accuracy than the best predictor in the ensemble.\n",
    "    - Even if each classifier is a weak learner (does slightly better then random guessing).\n",
    "        - Provided there are a sufficient number of weak learners and enough diversity.\n",
    "- Due to the law of large numbers, if you build an ensemble containing 1,000 classifiers with individual accuracies of $51%$ & trained for binary classification, If you predict the majority voting class, you can hope for up to $75%$ accuracy.\n",
    "    - This is only true if all classifiers are completely independent, making uncorrelated errors, which is clearly not the case because they are trained on the same data.\n",
    "- One way to get diverse classifiers is use different algorithms for each one of them & train them on different subset of the training data. \n",
    "- Let's implement a hard voting ensemble learner using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver='lbfgs')\n",
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "svm_clf = SVC(gamma='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=10000, noise=0.5)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6700, 2), (6700,), (3300, 2), (3300,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rf_clf), ('svc', svm_clf)], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)), ('rf', RandomF...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=None, voting='hard', weights=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's take a look at the performance of each classifier + ensemble method on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8084848484848485\n",
      "RandomForestClassifier 0.7993939393939394\n",
      "SVC 0.8212121212121212\n",
      "VotingClassifier 0.8193939393939393\n"
     ]
    }
   ],
   "source": [
    "for clf in [log_clf, rf_clf, svm_clf, voting_clf]:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_hat = clf.predict(X_val)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There you have it! The voting classifier slightly outperforms the individual classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If all ensemble method learners can estimate class probabilities, you can average their probabilities per class then predict the class with the highest probability.\n",
    "    - This is called **Soft voting**.\n",
    "    - It often yields results better than hard voting because it weights confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging & Pasting\n",
    "\n",
    "- Another approach to having different algorithms trained on the same dataset is to have one algorithm but trained on random subsets of the training data.\n",
    "    - When subset sampling is performed with replacement, this is called **bagging**.\n",
    "    - When sampling is performed without replacement, this is called **Pasting**.\n",
    "- Bagging is showcased in the following figure:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width: 50%;\" src=\"static/imgs/bagging.png\"></div>\n",
    "\n",
    "- Once the ensemble method is done training, we can infer using *mode* for classification or *avg* for regression.\n",
    "- Each individual predictor has a higher bias than if it were trained on the whole training dataset, but aggregation reduces both bias and variance.\n",
    "- Because ensemble learners use separate different algorithms, they can be easily parallalizable, and that is why bagging and pasting are so popular, they scale very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging & Pasting in Scikit-Learn\n",
    "\n",
    "- Let's implement bagging in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=100, n_estimators=500, n_jobs=-1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = bag_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging models often result in a slightly more biased model than the base predictor, but because of the diversity of the training subsets, it has much smoother decision curves meaning it has usually a lower variance\n",
    "- The comparable/slightly higher bias comes from the fact that we are sampling with replacement, leading to points making it to multiple child learners.\n",
    "- following is a comparison\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/dt_bagging.png\"></div>\n",
    "\n",
    "- Overall, bagging generally produce better models, and this explains why it's very popular.\n",
    "    - but if you have compute time and power you can use cross validation to test both bagging and pasting for your case and decide which works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag Evaluation\n",
    "\n",
    "- With bagging, some instances will be sampled several times for several predictors, while others won't be sampled at all.\n",
    "- Only about 63% of the instances are sampled for each predictor.\n",
    "- The other 37% of instances not sampled are called Out-of-Bag (oob) instances. \n",
    "    - Note that they're not the same 37% for all predictors.\n",
    "- Since the individual predictor never sees the oob samples, it can be evaluated on the oob data without the need for a separate validation set.\n",
    "- You can evaluate the ensemble itself by averaging the oob scores of each predictor.\n",
    "- Let's do this using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=0.63, bootstrap=True, n_jobs=-1, oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=0.63, n_estimators=500, n_jobs=-1, oob_score=True,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8052238805970149"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify this estimation using the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = bag_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7996969696969697"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Close enough!\n",
    "- The oob decision function is also available using scikit-learn's API, since our classifier is a decision tree then the decision function is a ratio that can be expressed as a probability, let's take a look at it for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61507937, 0.38492063],\n",
       "       [0.21126761, 0.78873239],\n",
       "       [0.01845018, 0.98154982],\n",
       "       ...,\n",
       "       [0.78181818, 0.21818182],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9962406 , 0.0037594 ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Patches & Random Subspaces\n",
    "\n",
    "- We can sample features as well.\n",
    "- Sampling is controlled by two hyper-parameters: `max_features` & `bootstrap_features`.\n",
    "- Thus, each predictor will be trained on a random feature sample.\n",
    "- This technique is especially useful when you're dealing with high-dimensional input as as imagery. \n",
    "- Sample both instances and features is called the random patches method.\n",
    "- Sampling only features is called the random subspaces method.\n",
    "- Sampling features results in a more predictor diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "- A random forest is an ensemble of decision trees.\n",
    "- Let's use its scikit-learn implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=16,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rnd_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following `BaggingClassifier` is roughly equivalent to the previous `RandomForestClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter='random', max_leaf_nodes=16),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra-Trees\n",
    "\n",
    "- When you are growing a tree in a random forest, only a subset of features are considered when splitting.\n",
    "- It's possible to make the trees more random by choosing different threshold of the features rather than searching for the best possible threshold.\n",
    "    - Such a model is called **Extremely Randomized Trees Ensemble**.\n",
    "- When don't really know if an extremely randomized trees model will or will not outperform a classical random forest model.\n",
    "    - Generally, the only way to know is to try both and check their results using cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "- Yet another great quality of random forests is that they make it so easy to measure the importance of each feature.\n",
    "- We measure the importance by averaging the reduced impurity of each node that uses a certain feature across all trees in the forest.\n",
    "    - More exactly it's a weighted average beacuse each node has a number of samples that is used to weight.\n",
    "- Let's get feature importance using random forests on the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.fit(iris['data'], iris['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10312810076831765\n",
      "sepal width (cm) 0.026382587657894523\n",
      "petal length (cm) 0.42063297937264393\n",
      "petal width (cm) 0.44985633220114357\n"
     ]
    }
   ],
   "source": [
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's plot pixel-wise MNIST feature importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.fit(digits['data'], digits['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(rnd_clf.feature_importances_.reshape(8,8), cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest are very handy to get a quick understanding of what features actually matter, particularly if you need to perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "- Boosting refers to any ensemble method that can combine several weak learners into a strong learner.\n",
    "- The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. \n",
    "- There are many boosting methods available, but by far the most popular one is **AdaBoost** (Adaptive boosting) and **gradient boosting**.\n",
    "- Let's start with Adaboost:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "- One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that were underfitted.\n",
    "- this results in the later predictors focusing more and more on the hard cases.\n",
    "- & This is the technique used by Adaboost.\n",
    "- The following figure demenstrates the process\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:50%;\" src=\"static/imgs/adaboost.png\"></div>\n",
    "\n",
    "- The algorithm first trains a base classifier and use it to make predictions on the training set. the algorithm then increases the weight of the missclassified instances and pass its weights + the data to the next predictor.\n",
    "- This iterative learning approach shares similarities with gradient descent, except in gradient descent we're using the gradients to minimize a cost function, but with AdaBoost we're simply adding more learners to the chain.\n",
    "- There is one important drawback of this method, It cannot be parallalized because each predictor needs the output of the previous predictor.\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
