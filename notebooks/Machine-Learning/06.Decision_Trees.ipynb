{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6. Decision Trees\n",
    "\n",
    "- In this Chapter, we will start by discussing how to train, validate, and make predictions with decision trees.\n",
    "- Then we will go through the CART training algorithm used by Scikit-Learn, we will discuss how to regularize trees and use them in regression tasks.\n",
    "- Finally, we will discuss some of the limitations of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Visualizing a Decision Tree\n",
    "\n",
    "- To understand decision trees, let's start by building one and taking a look at its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 2), (150,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris.data[:, 2:]  # Petal length and width\n",
    "y = iris.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can visualize the decision tree by using the `export_graphiz()` method to export a graph representation file then taking a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(tree_clf, \n",
    "                out_file='models/06/iris_tree.dot', \n",
    "                feature_names=iris.feature_names[2:],\n",
    "                class_names=iris.target_names,\n",
    "                rounded=True,\n",
    "                filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the graph file into a .png file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dot -Tpng models/06/iris_tree.dot -o static/imgs/iris_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here it is:\n",
    "\n",
    "<div style=\"text-align:center\"><img style=\"width:33%\" src=\"static/imgs/iris_tree.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "- To classify a new data point, you start at the root node of the graph (on the top), and you answer the binary questions and you reach the end leaf.\n",
    "    - That end leaf represents your class.\n",
    "        - It's really that simple!\n",
    "- One of the many qualities of decision trees is that they require very little data preparation.\n",
    "    - In fact, they don't require feature scaling or centering at all!\n",
    "- A node's `samples` attribute counts how many training instances are sitting on the node.\n",
    "- A node's `value` attribute tells you have many instances of each class are setting on the node.\n",
    "- A node's `gini` attribute measures the nodes impurity (pure == 0)\n",
    "- The following equation shows how the training algorithm computes the gini scores of the ith node:\n",
    "\n",
    "$$G_i=1-\\sum_{k=1}^n{p_{i,k}}^2$$\n",
    "\n",
    "- Where $p_{i,k}$ is the ratio of class $k$ instances among the training instances in that particular node.\n",
    "    - In our case: $k \\in \\{1,2,3\\}$.\n",
    "- Scikit-learn uses the CART algorithm, which produces only binary trees\n",
    "    - Non-leaf nodes only have two children\n",
    "- However, other algorithms such as ID3 can produce decision trees with nodes that have more than 2 children.\n",
    "- The following figure shows the decision boundaries of our decision tree\n",
    "    - Decision Trees tend to create lines/rectangles/boxes/.. and split the feature space linearly but iteratively.\n",
    " \n",
    "<div style=\"text-align:center\"><img style=\"width:50%\" src=\"static/imgs/decision_tree_boundaries.png\"></div>\n",
    "\n",
    "- Decision Trees are intuitive, and their predictions are easily interpretable.\n",
    "    - These types of models are called **white box** models.\n",
    "- In contrast, as we will see, Random Forests and Neural Networks are generally considered Black Box models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Class Probabilities\n",
    "\n",
    "- A decision tree can also estimate the probability that a certain instance belongs to a certain class.\n",
    "    - It just returns the ratio of that class over the sum of all instances in the leaf.\n",
    "- Let's check this in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interesting insight: you'll get the same probability as long as you're in a certain leaf box\n",
    "    - It doesn't matter if your new data point gets closer to the decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CART Training Algorithm\n",
    "\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
