{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6. Decision Trees\n",
    "\n",
    "- In this Chapter, we will start by discussing how to train, validate, and make predictions with decision trees.\n",
    "- Then we will go through the CART training algorithm used by Scikit-Learn, we will discuss how to regularize trees and use them in regression tasks.\n",
    "- Finally, we will discuss some of the limitations of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Visualizing a Decision Tree\n",
    "\n",
    "- To understand decision trees, let's start by building one and taking a look at its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 2), (150,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris.data[:, 2:]  # Petal length and width\n",
    "y = iris.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can visualize the decision tree by using the `export_graphiz()` method to export a graph representation file then taking a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(tree_clf, \n",
    "                out_file='models/06/iris_tree.dot', \n",
    "                feature_names=iris.feature_names[2:],\n",
    "                class_names=iris.target_names,\n",
    "                rounded=True,\n",
    "                filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the graph file into a .png file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dot -Tpng models/06/iris_tree.dot -o static/imgs/iris_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here it is:\n",
    "\n",
    "<div style=\"text-align:center\"><img style=\"width:33%\" src=\"static/imgs/iris_tree.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "- To classify a new data point, you start at the root node of the graph (on the top), and you answer the binary questions and you reach the end leaf.\n",
    "    - That end leaf represents your class.\n",
    "        - It's really that simple!\n",
    "- One of the many qualities of decision trees is that they require very little data preparation.\n",
    "    - In fact, they don't require feature scaling or centering at all!\n",
    "- A node's `samples` attribute counts how many training instances are sitting on the node.\n",
    "- A node's `value` attribute tells you have many instances of each class are setting on the node.\n",
    "- A node's `gini` attribute measures the nodes impurity (pure == 0)\n",
    "- The following equation shows how the training algorithm computes the gini scores of the ith node:\n",
    "\n",
    "$$G_i=1-\\sum_{k=1}^n{p_{i,k}}^2$$\n",
    "\n",
    "- Where $p_{i,k}$ is the ratio of class $k$ instances among the training instances in that particular node.\n",
    "    - In our case: $k \\in \\{1,2,3\\}$.\n",
    "- Scikit-learn uses the CART algorithm, which produces only binary trees\n",
    "    - Non-leaf nodes only have two children\n",
    "- However, other algorithms such as ID3 can produce decision trees with nodes that have more than 2 children.\n",
    "- The following figure shows the decision boundaries of our decision tree\n",
    "    - Decision Trees tend to create lines/rectangles/boxes/.. and split the feature space linearly but iteratively.\n",
    " \n",
    "<div style=\"text-align:center\"><img style=\"width:50%\" src=\"static/imgs/decision_tree_boundaries.png\"></div>\n",
    "\n",
    "- Decision Trees are intuitive, and their predictions are easily interpretable.\n",
    "    - These types of models are called **white box** models.\n",
    "- In contrast, as we will see, Random Forests and Neural Networks are generally considered Black Box models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Class Probabilities\n",
    "\n",
    "- A decision tree can also estimate the probability that a certain instance belongs to a certain class.\n",
    "    - It just returns the ratio of that class over the sum of all instances in the leaf.\n",
    "- Let's check this in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interesting insight: you'll get the same probability as long as you're in a certain leaf box\n",
    "    - It doesn't matter if your new data point gets closer to the decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CART Training Algorithm\n",
    "\n",
    "- Scikit-Learn uses the Classification and Regression Tree (CART) algorithm to train decision trees (also called \"growing\" trees).\n",
    "- The algorithm works by first splitting the training set by feature $k$ and threshold $t_k$.\n",
    "- How does it choose $k$ and $t_k$?\n",
    "    - It searches for $(k,t_k)$ that produce the purest subsets.\n",
    "        - Weighted by their size.\n",
    "- The following gives the loss function that CART tries to minimize:\n",
    "\n",
    "$$J(k,t_k)=\\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}$$\n",
    "\n",
    "- Where:\n",
    "    - $G_{left/right}$ measures the resulted impurity in the left/right subsets.\n",
    "    - $m_{left/right}$ correspond to the number of instances in the left/right subsets.\n",
    "- Once the CART algorithm successfully split the initial training data into two subsets, it does the same thing to both subsets.\n",
    "- It stops recursing once it reaches the maximum allowed tree depth (the `max_depth` hyper-parameter).\n",
    "    - Or if it cannot find a split that reduces impurity.\n",
    "- A few other hyper-parameters control stopping like:\n",
    "    - `min_samples_split`, `min_samples_leaf`, `min_weight_fraction_leaf`, `max_leaf_nodes`.\n",
    "- The CART algorithm is greedy in the sense that it doesn't care if its current split will lead to an optimal downstream leaf.\n",
    "    - It only cares about finding the best possible split at the current leaf.\n",
    "    - In that sense, it doesn't necessarily result in an optimal solution.\n",
    "- Unfortunately, finding the optimal tree is known to be an **NP-Complete** problem with a complexity of $O(exp(m))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Complexity\n",
    "\n",
    "- Making a prediction requires us to go from the root the final leaf.\n",
    "- Decision trees are approximately balanced, so traversing the decision tree require going through roughly $O(log_{2}(m))$.\n",
    "- Since each node requires check the value of only one feature, the overall inference running time is $O(log_{2}(m))$.\n",
    "    - Independent of the number of features.\n",
    "        - So predictions are really fast, even when dealing with a large number of features.\n",
    "- The training algorithm compares all features (except if `max_features` is set) on all samples at each node.\n",
    "- Comparing all features at all samples at each node results in a training complexity of $O(n \\times mlog_2(m))$.\n",
    "    - For small training sets (less than a few thousands), scikit-learn can speed up training by presorting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Impurity or Entropy?\n",
    "\n",
    "- In information theory, entropy is zero when all messages are identical.\n",
    "- In ML, entropy is often used as an impurity measure.\n",
    "- A set's entropy is zero when **it contains instances of only one class**.\n",
    "- The following formula shows the entropy at node $i$:\n",
    "\n",
    "$$H_i=-\\sum_{k=1}^{n}p_{i,k}log_2(p_{i,k})$$\n",
    "\n",
    "- There's no big difference between using Gini or Entropy to measure impurity.\n",
    "    - Gini impurity is slightly faster to compute.\n",
    "    - When they differ, Entropy tends to produce more balanced trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Hyperparameters\n",
    "\n",
    "- **Decision Trees make very few assumptions about the training data**.\n",
    "- If left unconstrained and because of its flexibility, a decision tree will adapt itself to perfectly fit the training data.\n",
    "    - Naturally leading to overfitting.\n",
    "- Such a model is often called a *non-parameteric model* because the number of parameters is not determined before training.\n",
    "- You can at least restrict the maximum depth of the decision tree.\n",
    "- Other regularization hyper-parameters include:\n",
    "    - `min_samples_split`: The minimum number of samples a node must have for it to split.\n",
    "    - `min_samples_leaf`: The minimum number of samples a leaf must have.\n",
    "    - `min_weight_fraction_leaf`: `mean_samples_leaf` as a fraction.\n",
    "    - `max_leaf_nodes`: the maximum number of leaf nodes.\n",
    "    - `max_features`: The maximum number of features that are evaluated for any split.\n",
    "- The following figure shows two decision trees trained on the same moon dataset, the left one represent an unconstrained trained decision tree, and the right one is regularized using the `min_samples_leaf` hyper-parameter:\n",
    "\n",
    "<div style=\"text-align:center\"><img style=\"width:50%\" src=\"static/imgs/regularized_tree.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "- ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
