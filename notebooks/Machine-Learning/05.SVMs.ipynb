{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. Support Vector Machines\n",
    "\n",
    "- SVM is a powerful and versetile ML model, capable of performing Classification, Regression, & even outlier detection.\n",
    "- SVMs are particularly suited for complex small-to-medium sized datasets.\n",
    "- This chapter will explain the core concepts of SVMs, how they work, and how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM Classification\n",
    "\n",
    "- The fundamental idea behind SVMs is better explained with the following picture:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/SVM_example.png\" /></div>\n",
    "\n",
    "- The two classes can be separated easily by a straight line (linearly separable).\n",
    "- The left plot shows the decision boundaries of three possible linear classifiers.\n",
    "- The dashed line model is so bad that it doesn't even separate the two groups linearly.\n",
    "- The other two models work perfectly on the plotted training set.\n",
    "    - But their boundaries are so close to the training data points that they'll probably not perform well on unseen data.\n",
    "- In constrast, the model on the right not only separates the training data linearly, it also stays as far as possible from both classes data points.\n",
    "    - Thus, it will likely perform well on unseen data.\n",
    "- You can think of an SVM as fitting the widest possible street (represented by the dashed lines) between the classes.\n",
    "    - This is called **Large Margin Classification**.\n",
    "- Notice that adding more training points off the street **won't effect the decision boundary at all**.\n",
    "- It's fully determined by the data points located at the edge of the street.\n",
    "- These instances are called **support vectors**.\n",
    "- SVMs are also sensitive to feature scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin Classification\n",
    "\n",
    "- If we restrict that all training instances should be off the street, this is called Hard Margin Classification, the problem with it is that is will only with Linearly separated data, and is greatly effected by the presence of outliers.\n",
    "- The following are two example of how outliers can mess-up hard margin classifiers:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/Hard_Margin_Classifier.png\" /></div>\n",
    "\n",
    "- To fix the issue, try to balance finding a wide street with limiting the number of violations.\n",
    "    - This is called **Soft Margin Classification**.\n",
    "- This can be controlled in scikit-learn by the `C` hyper-parameter:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/Soft_to_Hard_Street.png\" /></div>\n",
    "\n",
    "- By increasing `C`, We're increasing the sensitivity of the model to minimize margin violations within the training set.\n",
    "    - Meaning, If you're overfitting, try to reduce the value of the `C` hyper-parameter.\n",
    "- Let's use scikit-learn's SVMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 2), (150,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris['data'][:, [2,3]]  # Petal Length, Petal Width\n",
    "y = (iris['target'] == 2).astype(np.float64)  # Iris Virginica\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = Pipeline([\n",
    "    ('Scaler', StandardScaler()),\n",
    "    ('Linear_svc', LinearSVC(C=1, loss='hinge'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('Scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('Linear_svc',\n",
       "                 LinearSVC(C=1, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "                           penalty='l2', random_state=None, tol=0.0001,\n",
       "                           verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unlike Logistic Regression Models (with their sigmoid functions), SVMs do not output probabilities for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NonLinear SVM Classification\n",
    "\n",
    "- Many datasets are not even close to being lienarly separable.\n",
    "- One approach to handly non-linear modeling is to add more features, such as polynomial features.\n",
    "    - In some cases this can result in linearly separable datasets.\n",
    "- The following is an example of an original non-linearly separable dataset with only one feature $x_{1}$ (on the left), and an augmented linearly seprable dataset with an added feature $x_{2}=x_{1}^{2}$: \n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/nonlinear_to_linear.png\" /></div>\n",
    "\n",
    "- Let's implement this idea using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamedakramzaytar/opt/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('poly_features',\n",
       "                 PolynomialFeatures(degree=3, include_bias=True,\n",
       "                                    interaction_only=False, order='C')),\n",
       "                ('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 LinearSVC(C=10, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "                           penalty='l2', random_state=None, tol=0.0001,\n",
       "                           verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_svm_clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following represents the decision boundaries of the model, because we added polynomial degrees, projected boundaries are now non-linear:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:50%;\" src=\"static/imgs/polynomial_svms.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernels\n",
    "\n",
    "- At a low polynomial degrees, adding features cannot deal with complex datasets.\n",
    "- At a high polynomial degrees, we endup adding a lot of features, resulting in a very complex & slow model.\n",
    "- Fortunately, when using SVMs you can apply an almost miraculous mathematical technique called the kernel trick.\n",
    "    - The kernel trick makes it possible to have the same result as if you added many polynomial features without actually adding them.\n",
    "- Let's test it on the moon dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_kernel_svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', SVC(kernel='poly', degree=3, coef0=1, C=5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 SVC(C=5, cache_size=200, class_weight=None, coef0=1,\n",
       "                     decision_function_shape='ovr', degree=3,\n",
       "                     gamma='auto_deprecated', kernel='poly', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This model trains an SVM classifier using a kernel of third degree features.\n",
    "- If your model is overfitting, you might want to decrease the polynomial degree, and If it's underfitting, it might be a good idea to increase the degree\n",
    "    - `coef0` controls how much the model is influenced by high-degree polynomials vs. low degree polynomials.\n",
    "- The following showcases the previously trained model (on the left) vs. a more complex model of kernel degree 10:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/kernel_trick.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Features\n",
    "\n",
    "- Another technique to tackle non-linear problems is to add features computed using a **similarity function**.\n",
    "    - Which measures how much each instance resembles a particular landmark.\n",
    "- For example, let's take the 1D dataset discussed earlier & add two landmarks to it at $x_{1}=-2$ and $x_{1}=1$, as showcased in the left plot of:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/similarity_measures.png\" /></div>\n",
    "\n",
    "- We defined the similarity function to be the **Gaussian Radial Basis Function (RBF)** with $\\gamma = 0.3$:\n",
    "- $\\phi$ is a bell shaped function varying from 0 ($x$ is very far from $l$) to 1 ($x=l$).\n",
    "    - After defining a similarity function, the new features are the **distances** of each training instance from the landmarks.\n",
    "- As you can see from the plot on the right, the instances become lienarly separable using only distance features.\n",
    "- You may wonder how to select the landmarks\n",
    "    - The simplest approach is to create a landmark at each and every point of the training data.\n",
    "    - Doing that will increase the number of dimensions and will yield a better chance of finding a linear separator.\n",
    "    - The downside is that you will create additional `m` number of features (as rows), which may lead in performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian RBF Kernel\n",
    "\n",
    "- Just like the polynomial features method, the similarity features method can be useful in many ML algorithms, the problem is that with very large datasets, we'll endup with a very big feature space.\n",
    "- But once again we have the Kernel trick to make it look as if we added the additional features, let's do it with `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3, gamma=5,\n",
       "                     kernel='rbf', max_iter=-1, probability=False,\n",
       "                     random_state=None, shrinking=True, tol=0.001,\n",
       "                     verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's take a look at the predictions space with the training set instances (bottom left is the trained model above), others correspond to different hyper-parameter configurations:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/training_rbfs.png\" /></div>\n",
    "\n",
    "- Increasing $\\gamma$ makes the bell-shaped curve narrower, the decision boundary ends up being more irregular, wiggling around individual instances.\n",
    "    - So $\\gamma$ acts as a regularization hyper-parameter.\n",
    "        - Increasing gamma increases model sensitivity (may lead to overfitting)\n",
    "        - decreasing gamma increases model bias (may lead to underfitting)\n",
    "    - Similar to the `C` hyper-parameter.\n",
    "- Other kernels exist but used much more rarely.\n",
    "- Notes\n",
    "    - You should always try the linear kernel first\n",
    "    - If the training set is not too large, you should also try the gaussian RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Complexity\n",
    "\n",
    "- `LinearSVC` doesn't support the kernel trick and scales with time complexity of $\\mathcal{O}(m \\times n)$.\n",
    "    - The algorithm takes longer if you ask for higher precision.\n",
    "        - Precision is controlled by the tollerance hyper-parameter $\\epsilon$.\n",
    "- `SVC` is based on `libsvm` that supports the kernel trick with complexity between $\\mathcal{O}(m^{2} \\times n)$ & $\\mathcal{O}(m^{3} \\times n)$.\n",
    "    - This means that it gets dreadfuly slow when the training instances count gets big.\n",
    "    - This algorithm is good for small to medium sized datasets.\n",
    "    - It scales well with the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Regression\n",
    "\n",
    "- SVMs also support linear and nonlinear regression.\n",
    "- To move from classification to regression we reverse the objective\n",
    "    - Instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM regression tries to fit **as many instance as possible** on the street while limiting margin violations.\n",
    "- The width of the street is controlled by the hyper-parameter $\\epsilon$, following is an example:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/SVM_regression.png\" /></div>\n",
    "\n",
    "- Adding more instances into the margin doesn't effect the model's predictions; thus the model is said to be $\\epsilon$-insensitive.\n",
    "- Let's implement it using `sklearn`'s `SVR` (after scaling & centering the data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_reg = LinearSVR(epsilon=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,\n",
       "          intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "          random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To tackle linear regression tasks, we can use a kernelized SVM model.\n",
    "- Let's do it using the polynomial kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly_reg = SVR(kernel='poly', degree=2, C=100, epsilon=0.1, gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, cache_size=200, coef0=0.0, degree=2, epsilon=0.1, gamma='auto',\n",
       "    kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `LinearSVR` scales linearly with the size of the training set, while `SVR` is much slower (just like `LinearSVC` & `SVC`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under the Hood\n",
    "\n",
    "- This section explains how SVMs make predictions and how their training algorithms work, starting with Linear SVM classifiers.\n",
    "\n",
    "### Decision Function & Predictions\n",
    "\n",
    "- The linear SVM classifier model predicts the class of a new instance by simply computing $w^{T}x+b = w_1x_1+w_2x_2+\\dots+w_nx_n+b$, If it's positive, then $x$ is labeled $1$, otherwise $x$ is labeled $0$:\n",
    "\n",
    "$$\\hat{y}=\\begin{cases}\n",
    "1,  & \\text{if $w^{T}x+b \\ge 0$} \\\\\n",
    "0, & \\text{if $w^{T}x+b < 0$}\n",
    "\\end{cases}$$\n",
    "\n",
    "- The following showcases the training data points and the activation $w^{T}x+b$:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"static/imgs/SVM_space.png\" /></div>\n",
    "\n",
    "- Training an SVM Classifier is finding a $(w,b)$ that makes the margin as wide as possible while avoiding margin violations (hard margin) or limiting them (soft margin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Objective\n",
    "\n",
    "- We know that the slope of the decision function is $||w||$.\n",
    "- Dividing $||w||$ by $2$ will multiply the margin by $2$.\n",
    "- So we want to minimize $||w||$ to get a large margin.\n",
    "- If we also want to avoid any margin violations then:\n",
    "    - We want the decision function to be greater than 1 for all positive training instances.\n",
    "    - And lower than -1 for all negative training instances.\n",
    "- If we define $t^{(i)}=-1$ for negative instances (if $y^{(i)}=0$) and $t^{(i)}=1$ for positive instances (if $y^{(i)}=1$)\n",
    "    - then we can express this constraint as $t^{(i)}(w^{T}x^{(i)}+b)\\ge 1$\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
