{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Machine Learning Project\n",
    "\n",
    "- In this chapter you will work on a machine learning project end-to-end, pretending to be a recently hired data scientist at a real estate company.\n",
    "- Here are the main steps you will go through:\n",
    "    1. Frame the problem & look at the big picture\n",
    "        - [ ] Define the objective in Business terms\n",
    "        - [ ] How will your solution be used?\n",
    "        - [ ] What are the current solutions/workarounds?\n",
    "        - [ ] How should you frame this problem?\n",
    "            - Supervised/Unsupervised\n",
    "            - Batch/Online\n",
    "            - Instance-based/Model-based\n",
    "        - [ ] How should performance be measured?\n",
    "        - [ ] Is the performance measure aligned with the business objective?\n",
    "        - [ ] What would be the minimum performance needed to reach the business objective?\n",
    "        - [ ] What are comparable problems? can you use previous methods or tools?\n",
    "        - [ ] Is human expertise available?\n",
    "        - [ ] How would you solve the problem manually?\n",
    "        - [ ] List the assumptions you/others have made so far?\n",
    "        - [ ] Verify assumptions if possible\n",
    "    2. Get the data\n",
    "        - [ ] List the data you need and how much you need\n",
    "        - [ ] Find & Document where you can get that data\n",
    "        - [ ] Check how much space it will take\n",
    "        - [ ] Check legal obligations & get authorization if necessary\n",
    "        - [ ] Get access authorization\n",
    "        - [ ] Create a workspace with enough storage space\n",
    "        - [ ] Get the data\n",
    "        - [ ] Convert the data into a format you can easily manipulate\n",
    "        - [ ] Ensure sensitive information is either deleted or protected\n",
    "        - [ ] Check the size & type of data\n",
    "        - [ ] Sample a test set, put it aside, and never look at it\n",
    "    3. Explore the Data\n",
    "        - [ ] Create a copy of the data for exploration, sample it down if necessary\n",
    "        - [ ] Create a Jupyter notebook to keep records on your data exploration\n",
    "        - [ ] Study each attribute and its characteristics\n",
    "            - Name\n",
    "            - Type (Categorical, Continuous, Int/Float, Structured/Unstructured, Text ..)\n",
    "            - % of missing values\n",
    "            - Noisiness and type of noise (Stochastic, rounding error, ..)\n",
    "            - Usefulness for the task\n",
    "            - Type of distribution (Gaussian, Logarithmic, Uniform ..)\n",
    "        - [ ] For supervised Learning tasks, Identify the target attribute\n",
    "        - [ ] Visualize the data\n",
    "        - [ ] Study the correlations between attributes\n",
    "        - [ ] Study how you would solve the problem manually\n",
    "        - [ ] Identify the promising transformations you may want to apply\n",
    "        - [ ] Identify Extra data that would be useful\n",
    "        - [ ] Document what you have learned\n",
    "    4. Prepare the data for machine learning algorithms\n",
    "        - [ ] Create a copy of the data\n",
    "        - [ ] Write functions for all the data transformations you want to apply\n",
    "            - You can easily prepare the data the next time you get a fresh dataset\n",
    "            - You can apply these transformations in future projects\n",
    "            - You can clean and prepare the test set\n",
    "            - You can clean and prepare new data instances in production\n",
    "            - You can treat cleaning/processing steps as hyper-parameters.\n",
    "        - [ ] Data Cleaning\n",
    "            - Fix or remove outliers â€” Optional\n",
    "            - Fill in missing values (with 0, mean, median, inference, ...) or drop their rows/columns\n",
    "        - [ ] Feature Selection\n",
    "            - Drop the attributes that provide no useful information for the task\n",
    "        - [ ] Feature Engineering\n",
    "            - Discretize continuous features\n",
    "            - Decompose features (Categorical, datetime, ...)\n",
    "            - Add promissing feature transformations ($log(x)$, $sqrt(x)$, $x^{2}$, ..)\n",
    "            - Aggregate features into promising new features\n",
    "        - [ ] Feature scaling\n",
    "            - Standarize or normalize features\n",
    "    5. Shortlist promising models\n",
    "        - [ ] If the data set is big, sample smaller datasets for experimentation\n",
    "        - [ ] Try many models from different categories (NB, Linear regression, RF, NN, ..) using standard parameters.\n",
    "        - [ ] Measure and compare their performance\n",
    "            - For each model, measure N-fold cross validation and capture the mean and standard diviation of the performance.\n",
    "        - [ ] Analyze the most significant variable for each algorithm\n",
    "        - [ ] Analyze the types of errors the models make\n",
    "            - What data would a human use to avoid these errors?\n",
    "        - [ ] Perform a quick round of feature selection and engineering\n",
    "        - [ ] Perform one or two more quick iterations of the previous steps\n",
    "        - [ ] Shortlist the top-3 to 5 most performant algorithms that make different types of errors\n",
    "    6. Fine-tune your models & combine them into a great solution\n",
    "        - [ ] Use the whole dataset\n",
    "        - [ ] Fine-tune hyper-parameters using cross-validation\n",
    "            - Treat your data transformation choices as hyper-parameters\n",
    "            - Unless there are very few hyper-parameters to explore, prefer random search to grid search.\n",
    "                - If training takes a long time, you may prefer Bayesian Optimization.\n",
    "        - [ ] Try ensemble methods. Combining your best models will often produce better results than running them individually.\n",
    "        - [ ] Once you are confident about your model, measure its performance on the test set to estimate its generalization error.\n",
    "    7. Present your solution\n",
    "        - [ ] Document what you have done\n",
    "        - [ ] Create a nice presentation\n",
    "        - [ ] Explain why your solution achieves the business objective\n",
    "        - [ ] Showcase interesting things you noticed along the way\n",
    "        - [ ] Ensure your key findings are easily communicated through beautiful visualization and one-line statements\n",
    "    8. Launch, Monitor, and maintain your system\n",
    "        - [ ] Get your solution ready for production\n",
    "        - [ ] Write monitoring code to check your system's performance while running in production and run interval-based checks to alert when it drops.\n",
    "            - Beware of slow degradation: models tend to rot as data evolves\n",
    "            - Also monitor your inputs quality\n",
    "        - Re-train your model on a regular basis on fresh data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Real Data\n",
    "\n",
    "- When you are learning about ML, It's best to work with real data sets, not artificial ones.\n",
    "- Popular open data reposatories\n",
    "    - [UC Irvine ML repo](https://archive.ics.uci.edu/ml/index.php)\n",
    "    - [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "    - [Amazon AWS Datasets](https://registry.opendata.aws/)\n",
    "- Meta Portals: they list open data reposatories\n",
    "    - [Data Portals](http://dataportals.org/)\n",
    "    - [OpenDataMonitor](https://opendatamonitor.eu/frontend/web/index.php?r=dashboard%2Findex)\n",
    "    - [Quandl](https://www.quandl.com/)\n",
    "- Other pages listing many open data reposatories\n",
    "    - [Wikipedia](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research)\n",
    "    - [Quora](https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public)\n",
    "    - [The Datasets Subreddit](https://www.reddit.com/r/datasets)\n",
    "- In this chapter we'll use the California housing prices dataset, taken from the StatLib repository.\n",
    "- The dataset is based on data from the 1990 California cencus.\n",
    "- For teaching purposes we've added a categorical feature and removed multiple ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Look at the big picture\n",
    "\n",
    "- You first task is to use the california census data to build a model of the housing prices in the state.\n",
    "- This data includes metrics such as:\n",
    "    - Population\n",
    "    - Median Income\n",
    "    - Median housing price for each block group in California\n",
    "        - A block group is the smallest geographical unit for which cencus data is published\n",
    "            - A Block group has a population between 600 to 3,000.\n",
    "        - We will call them \"districts\" for short.\n",
    "- You model should be able to predict the median housing price for any district, given the other features.\n",
    "- Frame the problem\n",
    "    - The first question to ask your boss is: \"What is the business objective?\"\n",
    "        - Building a model is probably not the end goal.\n",
    "    - How does the comapny expects to benefit from the model?\n",
    "    - Your boss answers that your model's output (A prediction for a district's median housing price) will be fed along other signals to another model to decide whether or not to invest in the district.\n",
    "    - Getting this right is critical, as it's directly connects to revenue.\n",
    "    - A sequence of data processing components is called a **data pipeline**\n",
    "        - Pipelines are very common is machine learning systems, since data needs to be preprocessed, manipulated, transformed to output the final predictions.\n",
    "    - Components typically run asynchonosly\n",
    "    - Each component pulls in a large amount of data, process it and splits out a result (prediction) into a data store.\n",
    "    - Each component is fairly self-contained, the interface between components are the data stores.\n",
    "    - Different teams can focus on different components.\n",
    "    - The next question to ask your boss is \"what the current solution looks like?\"\n",
    "    - Answer the following questions $\\to$\n",
    "        - Is the problem supervised, unsupervised, or reinforcement learning?\n",
    "            - A supervised learning problem.\n",
    "        - Is it a classification or regression task?\n",
    "            - Since we're predicting the median housing price for a district, and since housing prices and numerical, this is a **Regression Task**\n",
    "        - Should you use batch learning or online learning techniques?\n",
    "            - Since cencus data is historical and does come every year, a **batch learning** approach is better.\n",
    "    - Note: if the data is huge, you can either split the data between multiple servers using Map Reduce or use online learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select a Performance Measure\n",
    "    - A typical performance measure for regression problems is **Root Mean Squared Error**. \n",
    "        - It has a higher weight for large errors\n",
    "        - Following is its general formula:\n",
    "$$ RMSE(X,h) = \\sqrt{ {1 \\over m} \\sum_{i=1}^m ( h(x^{(i)}) - y^{(i)} )^2  }$$\n",
    "- $m$ is the number of instances in the dataset you are measuring the $RMSE$ on.\n",
    "- $x^{(i)}$ is a vector containing all of the input feature values (excluding the label) for the $i^{th}$ instance.\n",
    "- $y^{(i)}$ is the label or the desired output of input $x^{(i)}$.\n",
    "- $X$ is a **matrix** containing all feature values excluding the labels/targets. The $i^{th}$ row of $X$ corresponds to $x^{(i)}$ and we can note:\n",
    "$$X=\\left(\n",
    "\\begin{array}{c}\n",
    "  {x^{(1)}}^{T}\\\\\n",
    "  {x^{(2)}}^{T}\\\\\n",
    "  {x^{(3)}}^{T}\\\\\n",
    "  \\vdots\\\\\n",
    "  {x^{(m)}}^{T}\\\\\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "- $h$ is your system's prediction function, also called hypothesis.\n",
    "- $RMSE(X,h)$ is the cost function measured on the set of examples $X$ and the hypothesis $h$.\n",
    "- $RMSE$ is the preferred performance measure for regression tasks\n",
    "    - But sometimes, we prefer to use other cost functions.\n",
    "- In a case where you have many outliers, you may consider using mean absolute error as a cost function ($MAE$).\n",
    "    - Also called the average absolute diviation.\n",
    "$$ MAE(X,h) = {1 \\over m} \\sum_{i=1}^m |h(x^{(i)}) - y^{(i)}| $$\n",
    "- Both $RMSE$ and $MAE$ are ways to measure the distance between two vectors, in our case, the distance between the vector of predictions and the vector of targets/labels.\n",
    "- Various distance measures, or norms, are possible:\n",
    "    - Computing the root of the sum of squares ($RMSE$) corresponds to the euclidian norm.\n",
    "        - It is also called $l_{2}$ norm.\n",
    "    - Computing the sum of absolutes ($MAE$) corresponds to the $l_{1}$ norm.\n",
    "    - More generally, the norm $l_{k}$ of a vector $\\bf{x}$ is:\n",
    "$$||v||_{k} = (\\sum_{i=1}^{m} |v_{i}|^{k})^{1 \\over k}$$\n",
    "        - $l_{0}$ gives the number of non-zero elements in the vector $v$\n",
    "        - $l_{\\infty}$ gives the maximum absolute value in the vector $v$\n",
    "- The higher the norm index, the more it focuses on large values and neglects small ones.\n",
    "    - This is why $RMSE$ is more sensitive to large errors than $MAE$.\n",
    "        - But when outliers are exponentially rare (like in a bell curve) $RMSE$ performs extremely well and is preferred over other cost functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the assumptions\n",
    "    - Lastly, it is good practice to list and verify the list of assumptions made by you or others, this can help you catch serious mistakes early one.\n",
    "    - Example: You've spent 6months working on an algorithm to predict the median housing price per district only to find out later that your predictions are being converted into categories (\"Cheap\", \"Medium\", \"Expensive\").\n",
    "        - In this case, It would've been better to work on the classification problem instead of a regression one, that would produce better prediction of the downstream system.\n",
    "    - Fortunately, you find out that the team actually need the actual median housing prices. Good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get the Data\n",
    "\n",
    "- It is preferable to create some util functions to automate the process of downloading/extracting web-based data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"data\", \"01\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    \"\"\"Creates `HOUSING_PATH`, Downloads & Extracts the contents of `HOUSING_URL` into `HOUSING_PATH`\n",
    "    \n",
    "    # Arguments:\n",
    "        housing_url, string: the download link\n",
    "        housing_path, string: where to download & extract data\n",
    "    \"\"\"\n",
    "    os.makedirs(name=housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(url=housing_url, filename=tgz_path)\n",
    "    housing_tgz = tarfile.open(name=tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's write a small function to load the data using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    \"\"\"Loads Housing data into a pandas dataframe.\n",
    "    \n",
    "    # Arguments:\n",
    "        housing_path: the path where `housing.csv` exists\n",
    "    \n",
    "    # Returns:\n",
    "        data, pd.DataFrame: the housing data as a pandas dataframe\n",
    "    \"\"\"\n",
    "    data_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take a quick look at the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each row represents one district.\n",
    "- There are 10 attributes\n",
    "    - `longitude`\n",
    "    - `latitude`\n",
    "    - `housing_median_age`\n",
    "    - `total_rooms`\n",
    "    - `total_bedrooms`\n",
    "    - `population`\n",
    "    - `households`\n",
    "    - `median_income`\n",
    "    - `median_house_value`\n",
    "    - `ocean_proximity`\n",
    "- the `info()` method is useful to take a quick look at the data, in particular\n",
    "    - How many rows in total\n",
    "    - How many NaNs per column\n",
    "    - Data types for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      "longitude             20640 non-null float64\n",
      "latitude              20640 non-null float64\n",
      "housing_median_age    20640 non-null float64\n",
      "total_rooms           20640 non-null float64\n",
      "total_bedrooms        20433 non-null float64\n",
      "population            20640 non-null float64\n",
      "households            20640 non-null float64\n",
      "median_income         20640 non-null float64\n",
      "median_house_value    20640 non-null float64\n",
      "ocean_proximity       20640 non-null object\n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are $20,640$ instances in the dataset.\n",
    "    - Which means that it is fairly small by machine learning standards.\n",
    "    - $207$ districts are missing the `total_bedrooms` attribute\n",
    "    - We will need to take care of this later.\n",
    "- All attributes are numerical, except `ocean_proximity`\n",
    "- Since we noticed repeated `ocean_proximity` values for the top 5 rows, we suspect that it is a categorical column, let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1H OCEAN     9136\n",
       "INLAND        6551\n",
       "NEAR OCEAN    2658\n",
       "NEAR BAY      2290\n",
       "ISLAND           5\n",
       "Name: ocean_proximity, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `.describe()` shows a summary of all numerical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20433.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>-119.569704</td>\n",
       "      <td>35.631861</td>\n",
       "      <td>28.639486</td>\n",
       "      <td>2635.763081</td>\n",
       "      <td>537.870553</td>\n",
       "      <td>1425.476744</td>\n",
       "      <td>499.539680</td>\n",
       "      <td>3.870671</td>\n",
       "      <td>206855.816909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2.003532</td>\n",
       "      <td>2.135952</td>\n",
       "      <td>12.585558</td>\n",
       "      <td>2181.615252</td>\n",
       "      <td>421.385070</td>\n",
       "      <td>1132.462122</td>\n",
       "      <td>382.329753</td>\n",
       "      <td>1.899822</td>\n",
       "      <td>115395.615874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-124.350000</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.499900</td>\n",
       "      <td>14999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>-121.800000</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1447.750000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>787.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>2.563400</td>\n",
       "      <td>119600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>-118.490000</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2127.000000</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>1166.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>3.534800</td>\n",
       "      <td>179700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>-118.010000</td>\n",
       "      <td>37.710000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>3148.000000</td>\n",
       "      <td>647.000000</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>605.000000</td>\n",
       "      <td>4.743250</td>\n",
       "      <td>264725.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>-114.310000</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>39320.000000</td>\n",
       "      <td>6445.000000</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>6082.000000</td>\n",
       "      <td>15.000100</td>\n",
       "      <td>500001.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longitude      latitude  housing_median_age   total_rooms  \\\n",
       "count  20640.000000  20640.000000        20640.000000  20640.000000   \n",
       "mean    -119.569704     35.631861           28.639486   2635.763081   \n",
       "std        2.003532      2.135952           12.585558   2181.615252   \n",
       "min     -124.350000     32.540000            1.000000      2.000000   \n",
       "25%     -121.800000     33.930000           18.000000   1447.750000   \n",
       "50%     -118.490000     34.260000           29.000000   2127.000000   \n",
       "75%     -118.010000     37.710000           37.000000   3148.000000   \n",
       "max     -114.310000     41.950000           52.000000  39320.000000   \n",
       "\n",
       "       total_bedrooms    population    households  median_income  \\\n",
       "count    20433.000000  20640.000000  20640.000000   20640.000000   \n",
       "mean       537.870553   1425.476744    499.539680       3.870671   \n",
       "std        421.385070   1132.462122    382.329753       1.899822   \n",
       "min          1.000000      3.000000      1.000000       0.499900   \n",
       "25%        296.000000    787.000000    280.000000       2.563400   \n",
       "50%        435.000000   1166.000000    409.000000       3.534800   \n",
       "75%        647.000000   1725.000000    605.000000       4.743250   \n",
       "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
       "\n",
       "       median_house_value  \n",
       "count        20640.000000  \n",
       "mean        206855.816909  \n",
       "std         115395.615874  \n",
       "min          14999.000000  \n",
       "25%         119600.000000  \n",
       "50%         179700.000000  \n",
       "75%         264725.000000  \n",
       "max         500001.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `.describe()` ignores null values.\n",
    "- The `std` row shows standard deviation, which measures how dispersed the values are.\n",
    "- The `25%`, `50%`, `75%` rows show the persentiles of each columns\n",
    "    - Example: 25% of districts have <=18 years housing median age.\n",
    "- Another way to get a feel of numerical continuous data is to draw a histogram for each numerical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
