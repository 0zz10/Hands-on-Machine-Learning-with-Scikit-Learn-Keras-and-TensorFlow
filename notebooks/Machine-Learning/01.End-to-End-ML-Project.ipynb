{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Machine Learning Project\n",
    "\n",
    "- In this chapter you will work on a machine learning project end-to-end, pretending to be a recently hired data scientist at a real estate company.\n",
    "- Here are the main steps you will go through:\n",
    "    1. Frame the problem & look at the big picture\n",
    "        - [ ] Define the objective in Business terms\n",
    "        - [ ] How will your solution be used?\n",
    "        - [ ] What are the current solutions/workarounds?\n",
    "        - [ ] How should you frame this problem?\n",
    "            - Supervised/Unsupervised\n",
    "            - Batch/Online\n",
    "            - Instance-based/Model-based\n",
    "        - [ ] How should performance be measured?\n",
    "        - [ ] Is the performance measure aligned with the business objective?\n",
    "        - [ ] What would be the minimum performance needed to reach the business objective?\n",
    "        - [ ] What are comparable problems? can you use previous methods or tools?\n",
    "        - [ ] Is human expertise available?\n",
    "        - [ ] How would you solve the problem manually?\n",
    "        - [ ] List the assumptions you/others have made so far?\n",
    "        - [ ] Verify assumptions if possible\n",
    "    2. Get the data\n",
    "        - [ ] List the data you need and how much you need\n",
    "        - [ ] Find & Document where you can get that data\n",
    "        - [ ] Check how much space it will take\n",
    "        - [ ] Check legal obligations & get authorization if necessary\n",
    "        - [ ] Get access authorization\n",
    "        - [ ] Create a workspace with enough storage space\n",
    "        - [ ] Get the data\n",
    "        - [ ] Convert the data into a format you can easily manipulate\n",
    "        - [ ] Ensure sensitive information is either deleted or protected\n",
    "        - [ ] Check the size & type of data\n",
    "        - [ ] Sample a test set, put it aside, and never look at it\n",
    "    3. Explore the Data\n",
    "        - [ ] Create a copy of the data for exploration, sample it down if necessary\n",
    "        - [ ] Create a Jupyter notebook to keep records on your data exploration\n",
    "        - [ ] Study each attribute and its characteristics\n",
    "            - Name\n",
    "            - Type (Categorical, Continuous, Int/Float, Structured/Unstructured, Text ..)\n",
    "            - % of missing values\n",
    "            - Noisiness and type of noise (Stochastic, rounding error, ..)\n",
    "            - Usefulness for the task\n",
    "            - Type of distribution (Gaussian, Logarithmic, Uniform ..)\n",
    "        - [ ] For supervised Learning tasks, Identify the target attribute\n",
    "        - [ ] Visualize the data\n",
    "        - [ ] Study the correlations between attributes\n",
    "        - [ ] Study how you would solve the problem manually\n",
    "        - [ ] Identify the promising transformations you may want to apply\n",
    "        - [ ] Identify Extra data that would be useful\n",
    "        - [ ] Document what you have learned\n",
    "    4. Prepare the data for machine learning algorithms\n",
    "        - [ ] Create a copy of the data\n",
    "        - [ ] Write functions for all the data transformations you want to apply\n",
    "            - You can easily prepare the data the next time you get a fresh dataset\n",
    "            - You can apply these transformations in future projects\n",
    "            - You can clean and prepare the test set\n",
    "            - You can clean and prepare new data instances in production\n",
    "            - You can treat cleaning/processing steps as hyper-parameters.\n",
    "        - [ ] Data Cleaning\n",
    "            - Fix or remove outliers â€” Optional\n",
    "            - Fill in missing values (with 0, mean, median, inference, ...) or drop their rows/columns\n",
    "        - [ ] Feature Selection\n",
    "            - Drop the attributes that provide no useful information for the task\n",
    "        - [ ] Feature Engineering\n",
    "            - Discretize continuous features\n",
    "            - Decompose features (Categorical, datetime, ...)\n",
    "            - Add promissing feature transformations ($log(x)$, $sqrt(x)$, $x^{2}$, ..)\n",
    "            - Aggregate features into promising new features\n",
    "        - [ ] Feature scaling\n",
    "            - Standarize or normalize features\n",
    "    5. Shortlist promising models\n",
    "        - [ ] If the data set is big, sample smaller datasets for experimentation\n",
    "        - [ ] Try many models from different categories (NB, Linear regression, RF, NN, ..) using standard parameters.\n",
    "        - [ ] Measure and compare their performance\n",
    "            - For each model, measure N-fold cross validation and capture the mean and standard diviation of the performance.\n",
    "        - [ ] Analyze the most significant variable for each algorithm\n",
    "        - [ ] Analyze the types of errors the models make\n",
    "            - What data would a human use to avoid these errors?\n",
    "        - [ ] Perform a quick round of feature selection and engineering\n",
    "        - [ ] Perform one or two more quick iterations of the previous steps\n",
    "        - [ ] Shortlist the top-3 to 5 most performant algorithms that make different types of errors\n",
    "    6. Fine-tune your models & combine them into a great solution\n",
    "        - [ ] Use the whole dataset\n",
    "        - [ ] Fine-tune hyper-parameters using cross-validation\n",
    "            - Treat your data transformation choices as hyper-parameters\n",
    "            - Unless there are very few hyper-parameters to explore, prefer random search to grid search.\n",
    "                - If training takes a long time, you may prefer Bayesian Optimization.\n",
    "        - [ ] Try ensemble methods. Combining your best models will often produce better results than running them individually.\n",
    "        - [ ] Once you are confident about your model, measure its performance on the test set to estimate its generalization error.\n",
    "    7. Present your solution\n",
    "        - [ ] Document what you have done\n",
    "        - [ ] Create a nice presentation\n",
    "        - [ ] Explain why your solution achieves the business objective\n",
    "        - [ ] Showcase interesting things you noticed along the way\n",
    "        - [ ] Ensure your key findings are easily communicated through beautiful visualization and one-line statements\n",
    "    8. Launch, Monitor, and maintain your system\n",
    "        - [ ] Get your solution ready for production\n",
    "        - [ ] Write monitoring code to check your system's performance while running in production and run interval-based checks to alert when it drops.\n",
    "            - Beware of slow degradation: models tend to rot as data evolves\n",
    "            - Also monitor your inputs quality\n",
    "        - Re-train your model on a regular basis on fresh data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Real Data\n",
    "\n",
    "- When you are learning about ML, It's best to work with real data sets, not artificial ones.\n",
    "- Popular open data reposatories\n",
    "    - [UC Irvine ML repo](https://archive.ics.uci.edu/ml/index.php)\n",
    "    - [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "    - [Amazon AWS Datasets](https://registry.opendata.aws/)\n",
    "- Meta Portals: they list open data reposatories\n",
    "    - [Data Portals](http://dataportals.org/)\n",
    "    - [OpenDataMonitor](https://opendatamonitor.eu/frontend/web/index.php?r=dashboard%2Findex)\n",
    "    - [Quandl](https://www.quandl.com/)\n",
    "- Other pages listing many open data reposatories\n",
    "    - [Wikipedia](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research)\n",
    "    - [Quora](https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public)\n",
    "    - [The Datasets Subreddit](https://www.reddit.com/r/datasets)\n",
    "- In this chapter we'll use the California housing prices dataset, taken from the StatLib repository.\n",
    "- The dataset is based on data from the 1990 California cencus.\n",
    "- For teaching purposes we've added a categorical feature and removed multiple ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Look at the big picture\n",
    "\n",
    "- You first task is to use the california census data to build a model of the housing prices in the state.\n",
    "- This data includes metrics such as:\n",
    "    - Population\n",
    "    - Median Income\n",
    "    - Median housing price for each block group in California\n",
    "        - A block group is the smallest geographical unit for which cencus data is published\n",
    "            - A Block group has a population between 600 to 3,000.\n",
    "        - We will call them \"districts\" for short.\n",
    "- You model should be able to predict the median housing price for any district, given the other features.\n",
    "- Frame the problem\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
